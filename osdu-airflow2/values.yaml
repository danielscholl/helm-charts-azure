################################################################################
# Specify the azure environment specific values
#
azure:
  tenant: <azure_tenant>
  subscription: <azure_subscription>
  resourcegroup: <azure_resourcegroup>
  identity: <azure_identity>
  identity_id: <azure_identity_id>
  keyvault: <azure_keyvault>
  appid: <azure_appid>

################################################################################
# Specify the azure environment specific values
#
appinsightstatsd:
  enabled: false
  aadpodidbinding: "osdu-airflow2-identity"
  image:
    repository: msosdu.azurecr.io
    tag: "1.0.0-OSDU"

#################################################################################
# Specify log analytics configuration
#
logAnalytics:
  isEnabled: "true"
  workspaceId:
    secretName: "central-logging"
    secretKey: "workspace-id"
  workspaceKey:
    secretName: "central-logging"
    secretKey: "workspace-key"
  image:
    repository: msosdu.azurecr.io
    tag: "1.0.0-OSDU"

airflowLogin:
  name: <airflowLogin_name> #<-- Default Airflow Web UI username

airflowAuthentication:
  username: admin
  keyvaultMountPath: /mnt/azure-keyvault/
  passwordKey: airflow-admin-password

################################################################################
# Specify any custom configs/environment values
#
customConfig:
  rbac:
    createUser: "True"

################################################################################
# Specify pgbouncer configuration
#
pgbouncer:
  enabled: true
  port: 6543
  max_client_connections: 3000
  airflowdb:
    name: airflow2
    host: <airflow_externalDatabase_host>
    port: 5432
    pool_size: 100
    user: <airflow_externalDatabase_user>
    passwordSecret: "postgres"
    passwordSecretKey: "postgres-password"

################################################################################
# Specify KEDA configuration
#
keda:
  version_2_enabled: true


################################################################################
# Specify the airflow configuration
#
airflow:
  version_1_Installed: false
  pgbouncer:
    ## if the pgbouncer Deployment is created
    ##
    enabled: false


  ##################################
  # Kubernetes Pod Operator config
  ##################################
  kubernetesPodOperator:
    namespace: airflow
  
  serviceAccount:
    name: airflow

  ###################################
  # Kubernetes - Ingress Configs
  ###################################
  ingress:
    enabled: <airflow_ingress_enabled>
    web:
      annotations:
        kubernetes.io/ingress.class: azure/application-gateway
        appgw.ingress.kubernetes.io/request-timeout: "300"
        appgw.ingress.kubernetes.io/connection-draining: "true"
        appgw.ingress.kubernetes.io/connection-draining-timeout: "30"
        cert-manager.io/cluster-issuer: letsencrypt-prod-dns
        # The certificate is created in app gateway in the format 'cert-{namespace}-{tls-secret-name}'
        # Here the secret name is osdu-certificate. This secret is created in osdu namespace.
        # The certificate name needs to be changed if the namespace or the secret name change.
        appgw.ingress.kubernetes.io/appgw-ssl-certificate: "cert-osdu-azure-osdu-certificate"
        cert-manager.io/acme-challenge-type: http01
      path: "/airflow2"
      host: <airflow_ingress_web_host>
      livenessPath: "/airflow2/health"
      tls:
        enabled: false
        secretName: osdu-certificate
      precedingPaths:
        - path: "/airflow2/*"
          serviceName: airflow2-web
          servicePort: 8080

  ###################################
  # Database - External Database
  ###################################
  postgresql:
    enabled: false
  externalDatabase:
    type: postgres
    host: airflow-pgbouncer #<-- Azure PostgreSQL Database host or pgbouncer host (if pgbouncer is enabled)
    user: <airflow_externalDatabase_user> #<-- Azure PostgreSQL Database username, formatted as {username}@{hostname}
    passwordSecret: "postgres"
    passwordSecretKey: "postgres-password"
    port: 6543
    database: airflow2

  ###################################
  # Database - External Redis
  ###################################
  redis:
    enabled: false
  externalRedis:
    host: <airflow_externalRedis_host> #<-- Azure Redis Cache host
    port: 6380
    passwordSecret: "redis"
    passwordSecretKey: "redis-queue-password"
    databaseNumber: 2 #<-- Adding redis database number according to the Redis config map https://community.opengroup.org/osdu/platform/deployment-and-operations/infra-azure-provisioning/-/blob/master/charts/osdu-common/templates/redis-map.yaml#L7

  ###################################
  # Airflow - DAGs Configs
  ###################################
  dags:
    installRequirements: true
    persistence:
      enabled: true
      existingClaim: airflow2dagpvc
      subPath: "dags"

  ###################################
  # Airflow - WebUI Configs
  ###################################
  web:
    replicas: 2
    livenessProbe:
      timeoutSeconds: 60
    resources:
      requests:
        cpu: "1000m"
        memory: "4Gi"
      limits:
        cpu: "1000m"
        memory: "4Gi"
    podLabels:
      aadpodidbinding: "osdu-airflow2-identity"
    podDisruptionBudget:
      enabled: true
      maxUnavailable: 1
    autoscale:
      enabled: false
      minReplicas: 2
      maxReplicas: 20
      scaleDown:
        coolDownPeriod: 60
    labels:
      # DO NOT DELETE THIS LABEL. SET IT TO "false" WHEN AUTOSCALING IS DISABLED, SET IT TO "true" WHEN AUTOSCALING IS ENABLED
      autoscalingEnabled: "false"
    baseUrl: "http://localhost/airflow"

  ###################################
  # Airflow - Worker Configs
  ###################################
  workers:
    resources:
      requests:
        cpu: "1200m"
        memory: "5Gi"
      limits:
        cpu: "1200m"
        memory: "5Gi"
    podLabels:
      aadpodidbinding: "osdu-airflow2-identity"
    podAnnotations:
      sidecar.istio.io/inject: "false"
    podDisruptionBudget:
      enabled: true
      maxUnavailable: 1
    # Use replicas when auto scaling is not enabled
    replicas: 6
    autoscale:
      enabled: false
      minReplicas: 2
      maxReplicas: 20
      scaleDown:
        coolDownPeriod: 300
    celery:
      gracefullTermination: true
      gracefullTerminationPeriod: 600
    labels:
      # DO NOT DELETE THIS LABEL. SET IT TO "false" WHEN AUTOSCALING IS DISABLED, SET IT TO "true" WHEN AUTOSCALING IS ENABLED
      autoscalingEnabled: "false"

  ###################################
  # Airflow - Flower Configs
  ###################################
  flower:
    enabled: false

  ###################################
  # Airflow - Scheduler Configs
  ###################################
  scheduler:
    replicas: 2
    resources:
      requests:
        cpu: "2500m"
        memory: "1Gi"
      limits:
        cpu: "2500m"
        memory: "1Gi"
    podLabels:
      aadpodidbinding: "osdu-airflow2-identity"
    podDisruptionBudget:
      enabled: true
      maxUnavailable: 1
    autoscale:
      enabled: false
      minReplicas: 2
      maxReplicas: 20
      scaleDown:
        coolDownPeriod: 60
    podAnnotations:
      sidecar.istio.io/inject: "false"
    variables: |
      {}
    
  ###################################
  # Airflow - Common Configs
  ###################################
  airflow:
    usersUpdate: null
    users: null
    image:
      repository: msosdu.azurecr.io/airflow2-docker-image
      tag: v2.2.4-v0.16-20220906-2
      pullPolicy: IfNotPresent
      pullSecret: ""
    config:
      AIRFLOW__SCHEDULER__STATSD_ON: "True"
      AIRFLOW__SCHEDULER__STATSD_HOST: "<statsd_endpoint>"
      AIRFLOW__SCHEDULER__STATSD_PORT: "8121"
      AIRFLOW__SCHEDULER__STATSD_PREFIX: "osdu_airflow2"
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "False"
      ## Enable for Debug purpose
      AIRFLOW__WEBSERVER__BASE_URL: https://<airflow_ingress_web_host>/airflow2
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "False"
      AIRFLOW__WEBSERVER__AUTHENTICATE: "True"
      AIRFLOW__WEBSERVER__AUTH_BACKEND: "airflow.contrib.auth.backends.password_auth"
      AIRFLOW__WEBSERVER__RBAC: "True"
      AIRFLOW__API__AUTH_BACKEND: "airflow.api.auth.backend.default"
      AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID: "az_log"
      AIRFLOW__LOGGING__REMOTE_LOGGING: "True"
      AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER: "wasb-airflowlog"
      AIRFLOW__LOGGING__LOGGING_CONFIG_CLASS: "log_config.DEFAULT_LOGGING_CONFIG"
      AIRFLOW__LOGGING__LOGGING_LEVEL: DEBUG
      AIRFLOW__LOGGING__LOG_FILENAME_TEMPLATE: "{{ run_id }}/{{ ti.dag_id }}/{{ ti.task_id }}/{{ ts }}/{% if dag_run.conf is not none and 'correlation_id' in dag_run.conf %}{{ dag_run.conf['correlation_id'] }}{% else %}None{% endif %}/{{ try_number }}.log"
      AIRFLOW__CELERY__SSL_ACTIVE: "True"
      AIRFLOW__WEBSERVER__ENABLE_PROXY_FIX: "True"
      AIRFLOW__CORE__PLUGINS_FOLDER: "/opt/airflow/plugins"
      AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: 60      
      AIRFLOW__WEBSERVER__WORKERS: 8
      AIRFLOW__WEBSERVER__WORKER_REFRESH_BATCH_SIZE: 0
      AIRFLOW__CORE__STORE_SERIALIZED_DAGS: True #This flag decides whether to serialise DAGs and persist them in DB
      AIRFLOW__CORE__STORE_DAG_CODE: True #This flag decides whether to persist DAG files code in DB
      AIRFLOW__WEBSERVER__WORKER_CLASS: sync
      AIRFLOW__CORE__PARALLELISM: "2000"
      AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG: "2000"
      AIRFLOW__CORE__DAG_CONCURRENCY: "2000"
      AIRFLOW__CELERY__WORKER_CONCURRENCY: "20" # Do not remove this config as it is used for autoscaling as well
      AIRFLOW__CORE__DAG_FILE_PROCESSOR_TIMEOUT: "1500"
      AIRFLOW_VAR_KEYVAULT_URI: https://<azure_keyvault>.vault.azure.net/
      AIRFLOW_VAR_CORE__CONFIG__SHOW_SKIPPED_IDS: true
    extraEnv:
      - name: CLOUD_PROVIDER
        value: "azure"
      - name: AIRFLOW__CORE__FERNET_KEY
        valueFrom:
          secretKeyRef:
            name: airflow
            key: fernet-key
      - name: AIRFLOW_CONN_AZ_LOG
        valueFrom:
          secretKeyRef:
            name: airflow
            key: remote-log-connection
      - name: AIRFLOW_VAR_AZURE_TENANT_ID
        valueFrom:
          secretKeyRef:
            name: active-directory
            key: tenantid
      - name: AIRFLOW_VAR_AZURE_CLIENT_ID
        valueFrom:
          secretKeyRef:
            name: active-directory
            key: principal-clientid
      - name: AIRFLOW_VAR_AZURE_CLIENT_SECRET
        valueFrom:
          secretKeyRef:
            name: active-directory
            key: principal-clientpassword
      - name: AIRFLOW_VAR_AAD_CLIENT_ID
        valueFrom:
          secretKeyRef:
            name: active-directory
            key: application-appid
      - name: AIRFLOW_VAR_APPINSIGHTS_KEY
        valueFrom:
          secretKeyRef:
            name: central-logging
            key: appinsights
      - name: AIRFLOW_VAR_AZURE_DNS_HOST
        value: <airflow_ingress_web_host>
      - name: AIRFLOW_VAR_AZURE_ENABLE_MSI
        value: <azure_enable_msi>
      - name: AIRFLOW_VAR_DAG_IMAGE_ACR
        value: <azure_acr>
      - name: PYTHONPATH
        value: "/opt/celery"
      - name: AIRFLOW_VAR_CORE__INGESTION__BATCH_SAVE_SIZE
        value: "500"
      - name: AIRFLOW_VAR_CORE__INGESTION__BATCH_COUNT
        value: "5"
      - name: AIRFLOW_VAR_CORE__INGESTION__BATCH_SAVE_ENABLED
        value: "true"
      ## Begin -- Ingest Manifest DAG variables
      - name: AIRFLOW_VAR_ENTITLEMENTS_MODULE_NAME
        value: "entitlements_client"
      - name: AIRFLOW_VAR_CORE__CONFIG__DATALOAD_CONFIG_PATH
        value: "/opt/airflow/dags/configs/dataload.ini"
      - name: AIRFLOW_VAR_CORE__SERVICE__SCHEMA__URL
        value: "http://schema.<namespace>.svc.cluster.local/api/schema-service/v1"
      - name: AIRFLOW_VAR_CORE__SERVICE__SEARCH__URL
        value: "http://search.<namespace>.svc.cluster.local/api/search/v2"
      - name: AIRFLOW_VAR_CORE__SERVICE__STORAGE__URL
        value: "http://storage.<namespace>.svc.cluster.local/api/storage/v2"
      - name: AIRFLOW_VAR_CORE__SERVICE__FILE__HOST
        value: "http://file.<namespace>.svc.cluster.local/api/file"
      - name: AIRFLOW_VAR_CORE__SERVICE__WORKFLOW__HOST
        value: "http://workflow.<namespace>.svc.cluster.local/api/workflow/v1"
      - name: AIRFLOW_VAR_CORE__SERVICE__DATASET__HOST
        value: "http://dataset.<namespace>.svc.cluster.local/api/dataset/v1"
      - name: AIRFLOW_VAR_CORE__SERVICE__SEARCH_WITH_CURSOR__URL
        value: "http://search.<namespace>.svc.cluster.local/api/search/v2/query_with_cursor"
      - name: AIRFLOW_VAR_CORE__SERVICE__PARTITION__URL
        value: "http://partition.<namespace>.svc.cluster.local/api/partition/v1"
      - name: AIRFLOW_VAR_CORE__SERVICE__LEGAL__HOST
        value: "http://legal.<namespace>.svc.cluster.local/api/legal/v1"
      - name: AIRFLOW_VAR_CORE__SERVICE__ENTITLEMENTS__URL
        value: "http://entitlements.<namespace>.svc.cluster.local/api/entitlements/v2"
      - name: AIRFLOW_VAR_ENV_VARS_ENABLED
        value: "true"
      ## End -- Ingest Manifest DAG variables
      - name: AIRFLOW__API__AUTH_BACKEND
        value: "airflow.api.auth.backend.basic_auth"
      - name: AIRFLOW_VAR_CORE__INGESTION__BATCH_SAVE_SIZE
        value: "500"
      - name: AIRFLOW_VAR_CORE__INGESTION__BATCH_COUNT
        value: "5"
      - name: AIRFLOW_VAR_CORE__INGESTION__BATCH_SAVE_ENABLED
        value: "true"
    extraPipPackages: [
        "flask-bcrypt==0.7.1",
        "apache-airflow[statsd,kubernetes,password]==2.2.4",
        "apache-airflow-providers-microsoft-azure==3.6.0",
        "google-cloud-storage",
        "python-keycloak==0.24.0",
        "msal==1.9.0",
        "azure-identity==1.5.0",
        "azure-keyvault-secrets==4.2.0",
        "azure-storage-blob",
        "azure-servicebus==7.0.1",
        "toposort==1.6",
        "strict-rfc3339==0.7",
        "jsonschema==3.2.0",
        "pyyaml==5.4.1",
        "requests==2.25.1",
        "tenacity==6.2.0",
        "authlib==0.15.4",
        "plyvel==1.3.0",
        "apache-airflow-providers-cncf-kubernetes==3.0.2"
      ]
    extraVolumeMounts:
      - name: azure-keyvault
        mountPath: "/mnt/azure-keyvault"
        readOnly: true
      - name: dags-data
        mountPath: /opt/airflow/plugins
        subPath: plugins
      - name: remote-log-config
        mountPath: /opt/airflow/config
        readOnly: true
      - mountPath: /opt/airflow/plugins/dag_runs_stats_plugin.py
        name: dag-runs-stats-plugin
        subPath: dag_runs_stats_plugin
    extraVolumes:
      - csi:
          driver: secrets-store.csi.k8s.io
          readOnly: true
          volumeAttributes:
            secretProviderClass: azure-keyvault-airflow
        name: azure-keyvault
      - configMap:
          name: airflow-remote-log-config
        name: remote-log-config
      - configMap:
          name: airflow-dag-runs-stats-plugin
        name: dag-runs-stats-plugin
    dbMigrations:
      podLabels:
        aadpodidbinding: "osdu-airflow2-identity"
    
    ###################################
    # Airflow - Comply with security policies
    ###################################
    containerSecurityContext:
      allowPrivilegeEscalation: false